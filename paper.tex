\documentclass{article}
\usepackage{epsf}
\usepackage[pdftex]{graphicx}
\usepackage[acronym]{glossaries}
\usepackage{booktabs}
\usepackage{subcaption}

\usepackage{enumitem}

\makeglossaries


\begin{document}


\title{A Bootstrapping Method for Dataset Construction}


\author{Oliver Batchelor, Richard Green}

\maketitle


\printglossaries
\newacronym{CNN}{CNN}{Convolutional Neural Network}
\newacronym{FCN}{FCN}{Fully Convolutional Network}
\newacronym{IOU}{IOU}{Intersection Over Union}
\newacronym{VOC}{VOC}{Visual Object Classes}
\newacronym{mIOU}{mIOU}{mean Intersection Over Union}
\newacronym{AMT}{AMT}{Amazon Mechanical Turk}
\newacronym{NLL}{NLL}{Negative Loss Likelyhood}
\newacronym{CRF}{CRF}{Conditional Random Field}



\begin{abstract}
 
\end{abstract} 
 


\section {Introduction}



One of the greatest problems faced in machine learning is obtaining accurate and sufficient annotated data. Deep learning has been shown to perform well on large datasets, and there exists a variety of large datasets in the public domain for research purposes. Applying deep learning to a new application presents a data collection and annotation problem, for which there are many solutions - and active research topics which we discuss below.

Not only is it necessary to collect and annotate data - one of the primary questions asked when considering the use of machine learning is 'How many examples are necessary?', to which the usual answer is to by experimental validation where part of the data is held back and used to the effectiveness of training on the other part.

We present a simple bootstrapping method for creating a segmentation dataset guided by a feedback loop using a human labeller and a model (a \gls{CNN}). We use segmentation as our task because it subsumes several simpler image annotation tasks, and is applicable to many domains. Our approach is based on using a partially trained model to assist the human annotator by, demonstrating the current level of knowlege transfer and providing a starting segmentation to work with.

We show that by using transfer learning by fine tuning a pre-trained model, very few hand annotated images are necessary to provide a good level of assistance to a human annotator, using the related idea that recognition is better than recall. Just as it is much faster to recognize a solution rather than recall one from memory, if a model can produce an annotation which is mostly correct - a human can recognize and fix small problems much faster than reproducing the annotation from scratch. At worst the human annotator can discard the automatic solution and produce the annotation by hand, and learn a measure of the model's current level.

\subsection{Related methods}



Human time becomes the main expense when annotating data, in order to minimise cost there exist strategies such as crowdsourcing, in order to get a lot of people to contribute for example as a game Quick Draw application \cite{Ha2017}, as a necessary task \cite{Goodfellow2013a}, or purchased at an enconomic global market with \gls{AMT}. Datasets such as ImageNet \cite{Russakovsky2015} are built with \gls{AMT}. 

One closely related method is active learning, the motto of active learning is ``putting the model in the loop'' (or conversely the human, depending on the perspective). Active learning requires a model with some estimate of uncertainty. Given a pool of non annotated examples, an algorithm can choose which examples require human annotation (because the model is uncertain), or go with the model's annotation (because the model is certain). One recent example, using active learning for segmentation is \cite{Xu2017}, where they focus on finding the nodes (super pixels) which induce the largest change in a \gls{CRF} model .

Semi-supervised machine learning attempts to use existing knowedge or domain properties to infer annotations. For example using motion cues to give indications of object boundaries \cite{Hong2017}, or using an image classifier to perform object detection by blanking out portions of the image, as to determine which parts are important \cite{Bazzani2016}. Semi-supervised methods often substitute for human labour, but in doing so make sacrifices of the quality and often result in somewhat noisy data. In our approach we look at a method to assist a human annotator to get the job done faster rather than attempt to fully automate a process with the goal to achieve a high quality dataset. 


\subsection {Assisted annotation}


\begin{figure*}[t]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/labelme.png}
  \caption{Labelme mask annotation}  
  \label{fig:labelme}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{images/superpixels.png}
  \caption{Annotation using super pixels}
  \label{fig:superpixels}
\end{subfigure}

\caption{Assisted annotation methods}
\label{fig:tree}
\end{figure*}


In our initial attempts to annotate a dataset, one of the problems was often that using the assisted annotation tools such as the mask tool in the LabelMe \cite{Russell2007} interface actually required more work than manually annotating the polygon boundary from scratch. The LabelMe mask tool provides a GrabCut \cite{Rother2004} interface where positive and negative stripes can be painted onto an image. Our experience in creating a mask for a tree trunk can be seen in figure \ref{fig:labelme} where annotations are bright red and green. The initial painted stripe provided a rough outline, but many small annotations were needed to make the mask resemble the true edge of the tree trunk.

In \cite{Galloway2017}, the images are segmented using superpixels (clusters of pixels grouped with an unspervised algorith), where labelling groups of pixels is substantially less work than labelling individual pixels, and superpixels seem to do well at finding image boundaries where \gls{CNN} are often imprecise. This does present some difficulties, such as the ambiguity where multiple classes are covered in one superpixel. In our experiences using superpixels provided imprecise boundaries and saved little time, an example of our tool with superpixel selection is shown in figure \ref{fig:superpixels}.

Deep learning has previous been used to aid in object instance selection. In \cite{Xu2016} a \gls{FCN} is trained to perform semantic segmentation of objects in images as selected by users. Users click on an object (providing positive and negative points), which are passed to the model as distance maps along with the regular colour image data. Another recent approach is that of \cite{Xu2017} which also aims at interactive instance segmentation, using a rectangle as an input (also using a distance map), and producing an instance segmentation.



\begin{figure*}[t]
  \centering
  \includegraphics[width=0.9\linewidth]{images/annotate.png}
  \caption{Annotation tool}  
  \label{fig:annotation}

\label{fig:tree}
\end{figure*}


\section{Approach}

Our approach is to annotate data and train a model as we go. First annotate a small number of examples to begin with to create an initial training set, and use these examples to train an initial model. New examples are then classified by the model, at which point the human annotater fixes any mistakes and the example is added to the training set. Initially the annotater will need to fix the majority of the data, and as the model improves will be less and less. The idea is that the model will quickly learn the easy cases, which can be quickly ignored to save work, and the annotater will be left only with the more important hard cases.

We have used this method to build a small scale tree trunk segmentation dataset, a prototype for the purposes of an automated drone tree pruning project for the purpose of pruning lower branches off young Pinus Radiata tree plantations. Our work is intended to allow the drone see the trees despite the forest. For the purposes of constructing the dataset, and for experimentation we built an annotation application shown in \ref{fig:annotation} written in Qt, with tools for drawing on masks and iterating the dataset, viewing existing annotations and finding images requiring annotation.

Our workflow after first training some initial number of examples was to leave a training process in the background which would pick up newly created annotations after each training epoch. 


\subsection {Models}

For our image segmentation model we have used an encoder--decoder ``ladder'' like architecture \gls{CNN}. The structure is similar to the UNet \cite{Ronneberger2015}, without cropping the skip connections.  Each level of the encoder and decoder (at the same resolution) are connected by a skip connection bypassing the lower layers of the network. We focus on two architectures, the first is a pretrained ResNet model as an encoder with a decoder bolted on, the second is a symmetrical encoder and decoder. We refer to the first as ``pretrained'' and the second as ``ladder'' in the experiments below.

The architecture based on the pretrained ResNet is shown in figure \ref{fig:network}, with the decoder and residual block used shown in figure \ref{fig:decode_block}. The particular ResNet we use is the ``resnet18'' from the Pytorch model zoo, (the smallest of the various ResNet architectures) with weights trained on the ImageNet dataset. 

Our decoder for the ``pretrained'' model uses a single residual block at the point labelled ``residual block(s)'' in figure \ref{fig:decode_block}. The ``ladder'' model uses residual blocks of size (1, 2, 3, 3, 3, 3) in both encoder and decoder. Our decoder uses $ 1x1 $ convolutions to reduce the feature sizes coming from intermediate layers of the ResNet, the number of features used in the decoder is 16, with 8 additional features at each layer further down. An implicit Batch Normalization layer and ReLU is present before every convolution.

Of note the first skip connection is directly the image input, and the second is directly after the $7x7, stride=2$ first convolution used by the ResNet.

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{images/network.pdf}
\caption{Encoder--decoder network with pretrained Resnet}  
\label{fig:network}
\end{figure*}


\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{images/network_blocks.pdf}
\caption{Block sub networks}  
\label{fig:decode_block}
\end{figure*}


\section {Experiments}


\subsection {Loss Function}

We optimize the Jaccard Distance as a continuous approximation to the \gls{IOU}. We use this loss fuction primarily because it avoids some of the problems with class imbalance which is present with the usual softmax-\gls{NLL} loss function over pixels. 

Segmentation datasets typically have a large class imabalance because the size of objects in pixels are uneven, in particular the number of background pixels is often much larger than the number of object pixels, and between images objects can differ vastly in size. This can present problems for training a model effectively, introduce local minimums and 

One way to solve imbalances is by applying class weighting (or pixel weighting for specific objects), in order to balance the importance of any particular object. We instead try to avoid this by optimising on the Jaccard distance, which matches our measure of success (\gls{IOU}) as well.


The \gls{IOU} is presented in equation \ref{eq:iou} with $ A $ and $ B $ being sets. The equivalent for binary vectors (pixels in the image), is given in equation \ref{eq:jaccard} where $ P $ is a binary prediction vector, which are image pixels output from the model and normalised using a sigmoid vector to be between 0 and 1. $ T $ is the binary target vector. Multiplication in these equations is element wise vector multiplication.

A related function is \ref{eq:dice} which is one minus the Dice coefficient, with the only difference being the term $ - | PT |_2 $ is missing from the denominator, such that true positives $ (A \cap B) $ are counted twice in the denominator.

\begin{equation}
IOU = \frac{A \cap B}{A \cup B}
\label{eq:iou}
\end{equation}



\begin{equation}
Jacc = 1 - \frac{| PT |}{| P^2 | + | T^2 | - | PT |}
\label{eq:jaccard}
\end{equation}


\begin{equation}
Dice = 1 - \frac{| PT |}{| P^2 | + | T^2 |}
\label{eq:dice}
\end{equation}



\subsection {Image preparation}

We use images of a fixed size in order to train our network (for the purposes of processing images in batches), however because our network is a fully convolutional network we can then test on images of variable size. We show the effect of this processing later, as compared to training with full size images with batches of size one.

Data augmentation is used to add variety. We use random scales (0.8 to 1.25), crops and rotations (-5 to 5 degrees). We adjust colours on a per colour channel basis ($ \gamma = 0.9 $ to $ \gamma=1.1 $ )  $ x_a = x^{\gamma} $.

After scaling and rotation, we then crop an area of the image of $440 \times 440$ pixels (the original image size in the trees dataset is $800 \times 600$, downscaled from the original photos of approximately 25 megapixels.

We employ image whitening as a last step, subtracting an approximate global mean (r, g, b) $ (0.485. 0.456, 0.406) $ and dividing by standard deviation $ (0.229, 0.224, 0.225) $ as to ensure consistency with the preprocessing used in ImageNet training 



\subsection {Datasets}

\begin{figure*}[t]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/trees_example.png}
  \caption{Typical example}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.9\linewidth]{images/trees_example_hard.png}
  \caption{A more difficult example}
  \label{fig:sub2}
\end{subfigure}

\caption{Example from the trees dataset}
\label{fig:tree}
\end{figure*}



We developed a small scale dataset for segmentation of tree trunks in Pinus Radiata forests (using the approach described). The dataset consisting of 120 images each containing many instances of tree. Of which we've split 20 images for a validation set for the purposes of experimentation in this paper. 

Along with our tree dataset, we've used classes from the Pascal VOC dataset in some experiments. We train with a mix of images from MS COCO \cite{Lin2014} training set along with the Pascal VOC 2012 training set, and use the VOC validation set for validation.


\subsection {Minimum viable examples}

The effectiveness of involving the model early in the process is going to be determined by the point fixing errors in the model's predictions is faster than annotating an image from scratch. 

The effect of number of examples was studied in \cite{Soekhoe} in the context of transfer learning. One finding was that with smaller numbers of examples, fixing the weights of earlier layers boosted performance when using less training examples.

We performed a simple validation experiment to investigate the effect of using less examples. The results can be seen in figure \ref{fig:limited}, in each case the stated IOU is an average over several trials (the number can be seen in table \ref{fig:limit_params}, and is additionally averaged over the last 3 epochs of training.

We can see that even for as few as 10 examples, the validation accuracy is not far off using the full dataset (1000s of images) in each case, and the difference between using 100 images and the full dataset in each case was very small. We expected to see the pretrained model perform much better with a limited number of examples, as it does - however it seems to just perform much better in the general case. The case matching our expectations is with the (much more limited) trees dataset.


\begin{table}[h]
  \centering
    \caption{Training parameters for limited examples experiment}

%\noindent\resizebox{\textwidth}{!}{%    
  \begin{tabular}{ l  l  l }
    Examples & Training epochs (of 256 images) & Repetitions \\
    \toprule
    1 	  & 8 	& 10 \\
    10 	  & 32 	& 4  \\
    100   & 64 	& 2 \\
    1000  & 128 & 1 \\
    \bottomrule
  \end{tabular}
%}
\label{fig:limit_params}
\end{table}



\begin{figure*}[t]
\centering
\includegraphics[width=0.9\linewidth]{images/limit.pdf}
\caption{Validation with a limited number of images, solid line is with the pretrained model}  
\label{fig:limited}
\end{figure*}



\subsection{Model finetuning}

We examined the best parameters for fine tuning, finding that allowing a small learning rate on the pretrained model was advantageous. The effects can be seen in figure \ref{fig:fine}, where $0.01$ (fraction of the learning rate used for other parts of the model) was advantageous. We used $ 0.1 $ for all other experimentation in this paper. It can be seen that without the lower learning rate, the pretrained parameters are disturbed leading to lower validation accuracy, and without any finetuning the pretrained parameters are not able to adapt.

Depth of the model is examined, we experiment with truncating the pretrained model. Results can be seen in figure \ref{fig:depth}, where the depth of the model was had a strong positive effect on the output of the model. Given these results we might consider using a model with even more depth scales, or using a lower resolution of input image. Given the number of convolutions (especially at the lower scales), the ResNet--18 model we have been using has quite a large receptive field - however in \cite{Peng2017} benefit was found in using very large kernels at the lower levels of the model specifically to enlarge the receptive field.

\begin{figure*}[t]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/fine_tuning.pdf}
  \caption{Influence of learning rate modifier for fine tuning}  
  \label{fig:fine}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=0.9\linewidth]{images/depth.pdf}
  \caption{Depth of pretrained model}  
  \label{fig:depth}
\end{subfigure}

\label{fig:training}
\end{figure*}




\subsection {Partial Annotation}


\begin{figure*}[h!]
\centering
\begin{subfigure}[t]{.3\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{images/loose.png}
  \caption{Partial annotation}
  \label{fig:loose_annot}
\end{subfigure}%
\begin{subfigure}[t]{.3\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{images/loose_directed.png}
  \caption{Directed annotation}
  \label{fig:loose_dir}

\end{subfigure}%
\begin{subfigure}[t]{.3\textwidth}
  \centering
  \includegraphics[width=0.8\linewidth]{images/loose_predictions.png}
  \caption{Predictions from partial annotation}
  \label{fig:loose_pred}
\end{subfigure}%
\label{fig:partial}
\end{figure*}



An alternative to densely annotating every pixel in an image is to selectively label some pixels while leaving many unlabelled - in the vein of the GrabCut algorithm. We examine this idea for a couple of reasons, firstly annotation all pixels is finicky or ambiguous (such as the small trees in the background), or secondly to optimize time spent on annotation only the most informative pixels (in the vein of active learning).

We firstly looked at partial annotation using paintbrush scribbles on the centre of trees and around the edges of the background. It can be seen that the model trained with partial annotation does not accurately find tree boundaries and has poor precision as a result as can be seen in figure \ref{fig:loose_pred}. After identifying flaws in that process we compared directed annotation where we trained a model as we went in order to guide the annotation process.

For directed annotation we densely labelled the tree instances where there were a mistakes, making sure to more accurately capture the boundaries. We were again annotating only a small number of easier instances where annotation was clear, unlike in the dense annotation case where many small and ambiguous instances are labelled.

Some statistics can be seen in table\ref{tab:loose_exp}, of note we can see that both methods of partial annotation are less effective than dense annotation. The second method of directed annotation improves on the precision, but sacrifices recall. We can see that the extra supervision is indeed useful, and we lose useful supervision by not validating the model's correct output as well as fixing it's mistakes.


\begin{table}[h]
  \centering
    \caption{Statistics from re-annotation test set}

\noindent\resizebox{\textwidth}{!}{%        
  \begin{tabular}{ l  l  l l l l l }
    Method & Positive pixels & Background pixels & Ignored pixels & Precision & Recall & IOU  \\
    \toprule
    Dense annotation	& 10.4\% & 89.6\% & 0\% & 87.8 & 87.3 & 77.8 \\
    Loose annotation	& 5.2\% & 67.4\% & 27.4\% & 82.3 & 83.3 & 70.7 \\
    Directed annotation & 7.2\% & 54.9\% & 38.0\% & 87.7 & 80.7 & 72.5 \\    
    \bottomrule
  \end{tabular}
}

\label{tab:loose_exp}
\end{table}






\subsection {Annotation interface experiment}

\begin{figure*}[h!]
\centering
\begin{subfigure}[t]{.3\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{images/line_tool.png}
  \caption{Line tool}
\end{subfigure}%
\begin{subfigure}[t]{.3\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{images/polygon_tool.png}
  \caption{Polygon tool}
\end{subfigure}%
\begin{subfigure}[t]{.3\textwidth}
  \centering
  \includegraphics[width=0.6\linewidth]{images/paint_tool.png}
  \caption{Paint tool}
\end{subfigure}%
\caption{Input methods for annotation}
\label{fig:tree}
\end{figure*}



We performed an exercise in reannotating the validation set using different tools. The results can be seen in table \ref{tab:annotation_exp}, we used three approaches. The different tools used can be seen above in figure \ref{fig:tools}.  The first being a duplication of the initial annotation using the line tool (the tool we initially used to label the dataset), the second being a polygon tool (more commonly used and general purpose than the line tool), and lastly we refined images produced by a trained model, fixing mistakes using the paint tool.

The model we used was a model trained on 30 images (densely labelled) from the loose annotation experiment above. We can see the method of refining model predictions is somewhat faster than the other two input methods, but due to the exercise being undertaken by a single user not significantly different ($ p > 0.01 $).

During the annotation we showed an image of the existing validation image and it's annotations side by side to the annotation software, this was necessary to ensure the same task was being performed. This was necessary as the smaller instances in each image were subject to the judgement of the annotator and other images were extremely crowded or blurry leading to ambiguity in the annotation process. 

One confounding feature of this task was the need to look backward and forward between the two images in order to ensure the correct areas were annotated. In images with many instances, this became a difficult task and identifying small differences between the two became much more difficult. In future we may perform a more comprehensive user study, by providing an outline of the desired mask for the user to copy directly. This way the user does not spend any time comparing images and can focus only on the interaction (as a normal user of the annotation would).

We can see from this exercise how noisy the tree dataset is, given that two annotations using the same tool and using the other as a reference guide only managed a precision and recall of approximately $ 90\% $. All three methods achieved a similar level of accuracy in this regard. 



\begin{table}[h]
  \centering
    \caption{Statistics from annotating validation set in different ways. Precision, recall and IOU are a comparison with the original validation set. Note figures in brackets are the original statistics of the un-modified predictions from the model}

\noindent\resizebox{\textwidth}{!}{%    
  \begin{tabular}{ l  l  l  l  l  l  l  l }
    Method & Time/image & Edits/image & Pixels/image & Time/edit & Precision & Recall & IOU \\
    \toprule
    Polygon tool	& $79.3 \pm 35.0$  & $12.1 \pm 5.6$ & $52438 \pm 31900$   & $3.9 \pm 1.8$  & $0.94$ & $0.88$   & $0.84$ \\
    Line tool 		& $71  \pm 33.4$   & $12.2 \pm 5.6$ & $54781 \pm 33336$   & $2.1 \pm 1.4$ & $0.92$  & $0.90$  & $0.84$ \\
    Refine from model 	& $57.3 \pm 40.3$  & $10.8 \pm 6.1$ & $6561 \pm 7102$ 	  & $1.7 \pm 1.4$ & $0.92 (0.89)$   & $0.92 (0.85)$ & $0.85 (0.77)$ \\    
    \bottomrule
  \end{tabular}
}

\label{tab:annotation_exp}
\end{table}




\subsection{Practical considerations}


The need for more traditional validation for determining hyper--parameters, (for example training rate, model architecture) still need to be determined by validation as soon as enough examples have been obtained to be feasible. We performed some tuning of the hyper--parameters (for example the crop size, data augmentation parameters, learning rate) for the purposes of the experiments in this paper using the person category of the Pascal \gls{VOC} 2012 dataset, despite the fact that the tree dataset is singnificantly faster to train a similar set of hyper--parameters seems to have been suitable.


A practical consideration of having a process training a \gls{CNN} in the background, is that it uses a lot of (both CPU and GPU) resources. Running the trainer and the annotation tool together caused significant lag in the annotation tool which was not otherwise present and especially noticable with a large batch size, the tool would lag in sync with each batch processed. The effect was that annotation became somewhat more time consuming. A simple solution to this would be to run processes on different machines.




\section {Conclusion}

We presented a simple bootstrapping method for dataset creation involving a feedback loop between human and model, where the model trained on partial data is used to assist a human annotator which then verifies the model's predictions and fixes any mistakes. We used this method to create a tree dataset for segmentation of Pinus Radiata trunks (for the purpose of finding trees requiring pruning). We then performed several experiments on this dataset, as well as the Pascal VOC in order to see how we might improve on this basic bootstrapping method. 

Through several experiments we determine that finetuning a pretrained model is an effective way to obtain a workable model with very few input images, at least on our tree dataset. Partial annotation was explored and found to be less effective than a dense annotation, for this reason we think that it is more useful to create tools to assist the user to create a dense annotation.


\section{Future work}


We see that our bootstrapping method is effective at a simple level, but offers several promising avenues which would make it a lot more useful. Given our experiment with partial annotation we would instead focus on methods which allow us to more effectively produce a dense annotation from a model's output plus a partial annotation. We implemented some preliminary ideas, for example GrabCut with RGB-P such as proposed in \ref{\cite{Xu2016a}, utilizing the model's predictions as another image channel.  Enhanced SuperPixels is another idea, however we found despite some improvement it suffered from many of the same problems as plain SuperPixels for small instances. For this reason we hypothesise that using a higher resolution may be beneficial in the annotation tool, even if our model is not trained at the full resolution.

Using an alternative pretrained model, such as those trained specifically on other segmentation tasks may be more effective. Using a model trained on so called mid level features, such as depth estimation, or models trained by metric learning, we hypothesise might provide a better alternative to the ImageNet model.


\bibliographystyle{acm}
\bibliography{library}
\end{document}
